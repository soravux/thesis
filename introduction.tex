%!TEX root = main.tex
\chapter*{Introduction}         % ne pas num√©roter
\phantomsection\addcontentsline{toc}{chapter}{Introduction} % inclure dans TdM

% General
Natural images are but a glimpse of captured light. They depict a morning in the park, some afternoon in the living room or any other scene that we can witness, yet they all resonate strongly with the human visual system. We are able to appreciate such images because evolution endowed us with high dynamic range wide-angle stereoscopic sensors, namely the eyes. But even equipped with those high-performance sensors, our complex nervous system choose to hallucinate most of what we see based on prior experience. These are only fragments of why automating the human visual system through computer vision tasks is so difficult. While the brain has access to sensors far superior to the currently available cameras, what makes it particularly complex is that for humans, the meaning of a pixel values is not only defined by its surroundings and context, but also by what happened in the past, as shaped by the decades of learning experienced by the observer.

As we begin to understand better our visual system, it becomes clear that, just like humans, complex vision tasks must rely on priors to be performed successfully. Those priors must be extracted from collected experience, using data as its own representation. Fortunately, the advent of social media has brought a phenomenal influx of images of all sorts each day to public databases, enabling the development of data-hungry machine learning algorithms such as deep neural networks. Such data-driven approaches are not only possible due to the newly available datasets, but also given the recent increase in computing speed and storage capacity of contemporary computers, allowing to handle the staggering amount of data publicly available nowadays. These machine learning methods bring a new paradigm to tackle vision problems: learning priors on natural images. These priors (initial beliefs on a probability distribution) are effectively additional constraints that can complement classical physics- or geometric-based approaches, which can improve solutions the of ill-posed problems.

This is the main topic of this dissertation: developing data-driven approaches to understand and solve problems that are ill-posed when considering them exclusively from a physics or geometric perspective. Specifically, two problems will be covered: the problem of 3D surface reconstruction using photometric cues, and lighting and camera calibration. In both problems, we are interested in cases when the environment is mostly uncontrolled, resulting in an under-constrained problem, impossible to solve robustly by classical approaches.

% First chapter
First, we will focus on recovering the 3D surface of an object, which can be done by photometric stereo (PS). It is a popular dense shape reconstruction technique that has matured extensively over nearly 40 years~\cite{woodham-opteng-80}. % to work with complex materials and lighting conditions~\cite{alldrin-cvpr-08,basri-ijcv-07,johnson-cvpr-11,oxholm-eccv-12}.
%Given the excellent PS results obtained in carefully designed laboratory setups,
Simply put, this technique proposes to recover the 3D surface normals of an object observed under varying illumination from a single viewpoint.
PS is reputed to give exceptional accuracy on surface normal estimation in fully calibrated environments, where lighting is controlled.

Recent investigations have turned to the more challenging problem of outdoor PS under uncontrolled, natural illumination. To do so, people used the sun as a point light source to model outdoor illumination. However, it turns out the sun follows a coplanar path throughout a single day, leaving the PS problem under-constrained. To solve this issue, recent approaches proposed to capture images over the course of many months~\cite{ackermann-cvpr-12,abrams-eccv-12}. This time interval provides enough shifting to the solar plane to constrain correctly the PS problem under the point light source assumption, although waiting for this amount of time is tedious and impractical.

% instead of trying to be invariant to natural illumination, 

In this thesis, we propose to leverage the richness of natural illumination to solve the outdoor photometric stereo problem in shorter time intervals. We will discover, as Jean Paul Richter puts it, ``a sky full of silent suns''. This brings us to our first main contribution:
%
\begin{quotation}
\textbf{Short-Term Photometric Stereo} We present a systematic analysis of the expected performance of PS algorithms in outdoor settings on a single day or less, and then propose data-driven approaches to solve the short-term PS problem under various weather conditions.
\end{quotation}

By using a richer lighting model than the point light source to solve the outdoor PS problem, we are able to understand why and when photometric cues can result in a stable surface reconstruction. This new knowledge allows us to develop approaches that bring outdoor photometric stereo one step closer to the reach of the masses, eventually transforming the sun into a powerful dense 3D scanner.

The other axis of research of this thesis explores an important step in most vision tasks, which is calibration. This is a typically tedious process that requires the detection of specific image features in many images. To simplify this process, one could perform their calibration using a single image of generic objects or features. However, this setup turns calibration into a severely ill-posed problem.

In this dissertation, we argue that machine learning can be used to learn priors on generic scenes and natural illumination. Using this additional information, it is possible to improve current single-image calibration techniques, leading to our second contribution:
%
\begin{quotation}
\textbf{Single Image Lighting and Camera Calibration} We present two single-image learning-based approaches to perform outdoor lighting and camera calibration. The proposed methods work on generic scenes with natural illumination and do not require a specific object to be present in the image.
\end{quotation}

Recent advances in machine learning allow for robust detection and classification of objects in images. By recognizing the objects present in a scene and comparing with their typical size in the physical world, one could estimate the focal length of the image. A similar process can be done with the object orientation and position, giving camera pitch and roll. Calibration would then be a matter of capturing the world by encoding common objects and generic scenes into a machine learning model.

Both contributions can have a direct impact on a vast amount of industries and projects. Surface reconstruction can allow the preservation of historical landmarks or cultural heritage like statues and architectural features in high-definition. It is also useful for the entertainment industry, where scanning of real-life models is critical for realism in video games. Single-image lighting and camera calibration enables automated photorealistic virtual object insertion, image alteration and relighting. As such, digital media industries are quite interested in those techniques. It could also be applied to forensics, allowing the analysis of potentially manipulated images.

\section*{Overview}

This thesis is grouped into two main axes: 1) surface reconstruction through photometric cues, and 2) learning-based lighting and camera calibration estimation. Both research axes are explored through a data-driven paradigm, each project leveraging between thousands and millions of images to train models by optimizing several millions of parameters, executing trillions of floating-point operations within seconds to process data yielding hundreds of thousands of dimensions. It would be a euphemism to say that the presented research could not have been performed without the recent advances in both computing and storage.

First, chapter~\ref{ch1} provides an in-depth analysis of the information contained in photometric cues throughout a single day and gives performance bounds on Photometric Stereo when performed on intervals down to a single hour. Upon investigation, we found that partially cloudy days brought enough constraints to solve the PS problem using an adapted PS algorithm. However, we show that sunny days in general are lacking the constraints to provide a stable surface reconstruction. In chapter~\ref{ch2}, we go one step further than what is possible using exclusively photometric cues. Since there is not enough information in a single sunny day to do a stable surface reconstruction solely from photometric cues, we employ a deep learning model to learn priors on local surface geometry and sun trajectory patterns. This additional information brings enough supplemental constraints to the PS problem to allow stable surface reconstructions.

Chapters~\ref{ch3} and~\ref{ch4} propose to extract priors from large datasets and utilize them to improve single-image outdoor lighting and camera calibration, respectively. Most current outdoor lighting estimation techniques rely exclusively on handcrafted features, limiting their application to a determined environment, for example urban scenes~\cite{lalonde-ijcv-12}. By learning features on scenes devoid of specific content, we propose a lighting estimation approach that is robust to generic scenes. Finally, the last chapter covers the problem of camera calibration from a single image. An emphasis is put on extrinsic calibration with respect to the earth, with a focus on finding the horizon within the image, even when hidden within the image. We further analyze human tolerance to errors on those calibration parameters.



%\bibliographystyle{abbrvnat}
%\bibliography{library}

